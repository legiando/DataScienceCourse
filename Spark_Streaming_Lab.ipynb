{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Streaming Lab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "kvD4HBMi0ohY",
        "b4Kjvk_h1AHl",
        "Y_2Cd36sWuvN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/legiando/DataScienceCourse/blob/master/Spark_Streaming_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3T3_FYC4ZBBE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spark Streaming Laboratory\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kvD4HBMi0ohY"
      },
      "cell_type": "markdown",
      "source": [
        "## Spark Bootstraping for Google Colab\n",
        "\n",
        "Run this before start working with the notebooks from the spark course. \n",
        "When you will start a new (and fresh) notebook at Colab. Google Cloud will create a new Docker container just for your use. \n",
        "\n",
        "Executing this notebook will install into the container the software. The container will be reused by the user until it will destroy by inactivity.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fUhBhrGmyAvs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless netcat -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "b4Kjvk_h1AHl"
      },
      "cell_type": "markdown",
      "source": [
        "## Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8Xnb_ePUyQIL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] pyspark-shell\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y_2Cd36sWuvN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cloning our Github repo"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PZkw_gPEQvId",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf /content/SparkCourse2019/\n",
        "!git clone https://github.com/bazarum/SparkCourse2019.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NwU28K5f1H3P"
      },
      "cell_type": "markdown",
      "source": [
        "## Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zgReRGl0y23D",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "print(findspark.init())\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "# from pyspark import SparkContext\n",
        "# sc = SparkContext(master = 'local[2]')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mzRJq0vPIDZ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## We will need Tweepy "
      ]
    },
    {
      "metadata": {
        "id": "QUACsloC5xhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tweepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "at2XmgJKIKCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This will be our tweets extractor\n"
      ]
    },
    {
      "metadata": {
        "id": "WjRimuj9Dj_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, print_function\n",
        "\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "\n",
        "consumer_key = 'w8TL68073oA9fBB8uY4jmXQ1C'\n",
        "consumer_secret = 'mWkBQcUKu9MFUAGSVOUjAv6EIKHmEwOLoohVYXdTEoq8glcKO1'\n",
        "access_token = '870541-Xxzh1Oke7Y4VqWxU6mqjO441TPTJ0xwmE3IjAYjXsLJ'\n",
        "access_token_secret = 'INTa3rPviJQ0AhAhK4vEceuTo2OkBxAK00f9J7YrWYEd7'\n",
        "\n",
        "class StdOutListener(StreamListener):\n",
        "    \"\"\" A listener handles tweets that are received from the stream.\n",
        "    This is a basic listener that just prints received tweets to stdout.\n",
        "\n",
        "    \"\"\"\n",
        "    def on_data(self, data):\n",
        "        print(data)\n",
        "        return True\n",
        "\n",
        "    def on_error(self, status):\n",
        "        print(status)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    l = StdOutListener()\n",
        "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "    stream = Stream(auth, l)\n",
        "    stream.filter(track=['selfie'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nE-lR_qTIXXw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now we can start getting some insights"
      ]
    },
    {
      "metadata": {
        "id": "vclXyV02meWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " df = spark.read.json('/content/SparkCourse2019/datasets/tweets/tweets.json', )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSi_dKbmmpgx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df.show() \n",
        "df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNQLl9dLnKSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "result = df.select(f.col(\"text\"), f.col(\"entities.media.media_url\"))\n",
        "result2 = df.select([\"text\", \"entities.media.media_url\"])\n",
        "result.show(10)\n",
        "result.printSchema()\n",
        "print(result.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAXzmKGP8pOJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"tweets\")\n",
        "result3 = spark.sql(\"SELECT text, entities.hashtags.text as hashtags, entities.media.media_url[0] as picture FROM tweets\")\n",
        "result3.show(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TP86-wc9Ba_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "allhashes = result3.withColumn(\"hashtag\", explode(result3.hashtags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1RicM2P2CJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "allhashes.createOrReplaceTempView(\"hashtags\")\n",
        "hashes = spark.sql(\"SELECT hashtag, count(*) as tot FROM hashtags GROUP BY hashtag ORDER BY tot DESC LIMIT 20\")\n",
        "hashes.show(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dSuIe408JOJa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra ball: how terminal access to the colab"
      ]
    },
    {
      "metadata": {
        "id": "WUwqvVBxK97M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Generate root password\n",
        "import random, string\n",
        "password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(20))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eoKp9_5LZPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Download ngrok\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "akqy-kbrLZFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Setup sshd\n",
        "! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "#Set root password\n",
        "! echo root:$password | chpasswd\n",
        "! mkdir -p /var/run/sshd\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc\n",
        "! echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_E4YqPhPLY7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Ask token\n",
        "print(\"Copy authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('export USER=root && ./ngrok authtoken $authtoken')\n",
        "get_ipython().system_raw('export USER=root && ./ngrok tcp 22 &')\n",
        "#Print root password\n",
        "print(\"Root password: {}\".format(password))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fi29rpMuSEBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! export USER=root && ./ngrok authtoken 7ta2zQwaM7BNPiDYpacrd_6gNx9yPBvpA7dy3DTc7NJ && ./ngrok tcp 22 &"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYHaUdNDN-nK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!# export USER=root && ./ngrok tcp 22 &\n",
        "!ps ax | grep ngrok\n",
        "!kill -9 1771"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xAnqpyV4LYw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get public address\n",
        "! curl -s http://localhost:4040/api/tunnels  # | python3 -c \\\n",
        "     # \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQocu75sLYN8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7sfsWk6JNph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra Ball 2: How to connect your drive "
      ]
    },
    {
      "metadata": {
        "id": "7Q2FoBoEMzke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ML3fFLzNDj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -lt \"drive/My Drive\" | head -10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OyH4Xj5wnMCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Streaming from a Socket"
      ]
    },
    {
      "metadata": {
        "id": "anEyDm7dnLvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json, split\n",
        "\n",
        "\n",
        "tweets = spark.readStream.format(\"socket\").option(\"host\", \"0.tcp.ngrok.io\").option(\"port\", 14439).load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfJ0ftnVFWY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68zSH-e7FP3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
        "lines = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"socket\") \\\n",
        "    .option(\"host\", \"0.tcp.ngrok.io\") \\\n",
        "    .option(\"port\", 14439) \\\n",
        "    .load()\n",
        "\n",
        "# Split the lines into words\n",
        "words = lines.select(\n",
        "   explode(\n",
        "       split(lines.value, \" \")\n",
        "   ).alias(\"word\")\n",
        ")\n",
        "\n",
        "# Generate running word count\n",
        "wordCounts = words.groupBy(\"word\").count()\n",
        "words = lines.select(\n",
        "   explode(\n",
        "       split(lines.value, \" \")\n",
        "   ).alias(\"word\")\n",
        ")\n",
        "\n",
        "# Generate running word count\n",
        "wordCounts = words.groupBy(\"word\").count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30T2zbijFrnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "query = wordCounts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fy3cbWN9njru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json\n",
        "\n",
        "data = spark.read.json(tweets.rdd.map(lambda r: r.json))\n",
        "\n",
        "# data = tweets.select(from_json(tweets.value).alias(\"dat\"))\n",
        "\n",
        "\n",
        "data.printSchema()\n",
        "\n",
        "\n",
        "# total = data.groupBy(\"line\").count()\n",
        "\n",
        "query = data.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
        "query.awaitTermination()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y7nt654S6wEy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwQzUDTsJPlA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "jGDikg5QIi8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init('/Users/donghua/spark-2.3.2-bin-hadoop2.7')\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Not required if running in Pyspark integrated notebooksc = SparkContext()\n",
        "\n",
        "ssc = StreamingContext(sc, 10)\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "socket_stream = ssc.socketTextStream(\"127.0.0.1\",5555)\n",
        "\n",
        "\n",
        "lines = socket_stream.window(20)\n",
        "\n",
        "from collections import namedtuple\n",
        "fields = (\"tag\",\"count\")\n",
        "Tweet = namedtuple('Tweet', fields)\n",
        "\n",
        "# use () for multiple lines(lines.flatMap(lambda text: text.split(\" \"))\n",
        ".filter(lambda word: word.startswith(\"#\"))\n",
        ".map(lambda word: (word.lower(), 1))\n",
        ".reduceByKey(lambda a, b : a + b)\n",
        ".map(lambda rec: Tweet(rec[0],rec[1]))\n",
        ".foreachRDD(lambda rdd: rdd.toDF().sort(desc(\"count\"))\n",
        ".limit(10).registerTempTable(\"tweets\")))\n",
        "\n",
        "\n",
        "import time\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas\n",
        "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "ssc.start()\n",
        "\n",
        "count = 0\n",
        "while count < 10:\n",
        "    time.sleep(10)\n",
        "    top_10_tweets = sqlContext.sql(\"select tag,count from tweets\")\n",
        "    top_10_df = top_10_tweets.toPandas()\n",
        "    display.clear_output(wait = True)\n",
        "    plt.figure(figsize= (10, 8))\n",
        "    sns.barplot(x=\"count\", y=\"tag\",data=top_10_df)\n",
        "    plt.show()\n",
        "    count = count+1\n",
        "ssc.stop()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4fCuDZxblE0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "squery = spark.readStream.format(\"rate\").load\n",
        "\n",
        "\n",
        "da\n",
        "\n",
        "# data = squery.writeStream.format(\"console\").start()\n",
        "                                                    \n",
        "# data.awaitTermination()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}